{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input, Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "img_rows, img_cols = 28, 28\n",
    "img_channels = 1\n",
    "num_pixels = img_cols*img_rows\n",
    "num_classes = 10\n",
    "num_trainImages = 42000\n",
    "num_testImages = 28000\n",
    "\n",
    "yTrain = np.ones((num_trainImages))\n",
    "xTrain = np.ones((num_trainImages,img_channels,img_cols,img_rows))\n",
    "counter = 0\n",
    "skip = True\n",
    "\n",
    "# train.csv should be in the same folder as this file\n",
    "trainFile = open('datasets/train.csv')\n",
    "csv_file = csv.reader(trainFile)\n",
    "for row in csv_file:\n",
    "    if (skip == True):\n",
    "        skip = False\n",
    "        continue\n",
    "    yTrain[counter] = row[0]\n",
    "    temp = np.ones((1,num_pixels))\n",
    "    for num in range(1,num_pixels):\n",
    "        temp[0,num - 1] = row[num]\n",
    "    temp = (temp - np.mean(temp))/(np.max(temp) - np.min(temp))\n",
    "    temp = np.reshape(temp, (img_rows,img_cols))\n",
    "    xTrain[counter,0,:,:] = temp\n",
    "    counter += 1\n",
    "\n",
    "yTest = np.ones((num_testImages))\n",
    "xTest = np.ones((num_testImages,img_channels,img_cols,img_rows))\n",
    "skip2 = True\n",
    "counter2 = 0\n",
    "\n",
    "testFile = open('datasets/test.csv')\n",
    "csv_file2 = csv.reader(testFile)\n",
    "for row in csv_file2:\n",
    "    if (skip2 == True):\n",
    "        skip2 = False\n",
    "        continue\n",
    "    yTest[counter2] = row[0]\n",
    "    temp = np.ones((1,num_pixels))\n",
    "    for num in range(1,num_pixels):\n",
    "        temp[0,num - 1] = row[num]\n",
    "    temp = (temp - np.mean(temp))/(np.max(temp) - np.min(temp))\n",
    "    temp = np.reshape(temp, (img_rows,img_cols))\n",
    "    xTest[counter2,0,:,:] = temp\n",
    "    counter2 += 1\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "yTrain = np_utils.to_categorical(yTrain, num_classes)\n",
    "yTest = np_utils.to_categorical(yTest, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK ARCHITECTURE\n",
    "input_img = Input(shape= (img_channels,img_rows, img_cols))\n",
    "x = Conv2D(32, kernel_size = (3,3), padding = 'Same', activation='relu')(input_img)\n",
    "x = Conv2D(64,  kernel_size = (3,3), padding = 'Same', activation='relu')(x)\n",
    "x = Conv2D(64,  kernel_size = (3,3), padding = 'Same', activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size=(1,1))(x)\n",
    "x = Flatten()(x)\n",
    "predictions = Dense(64, activation='relu')(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(predictions)\n",
    "\n",
    "model = Model(input_img, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT THE DATASET\n",
    "xTrain.shape\n",
    "vTrain = xTrain[33600:42000,:,:,:]\n",
    "vTest = yTrain[33600:42000,:]\n",
    "xTrain = xTrain[:33600,:,:,:]\n",
    "yTrain = yTrain[:33600,:]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TRAINING with TENSORBOARD\n",
    "class TrainValTensorBoard(TensorBoard):\n",
    "    def __init__(self, log_dir='./logs', **kwargs):\n",
    "        # Make the original `TensorBoard` log to a subdirectory 'training'\n",
    "        training_log_dir = os.path.join(log_dir, 'training')\n",
    "        super(TrainValTensorBoard, self).__init__(training_log_dir, **kwargs)\n",
    "\n",
    "        # Log the validation metrics to a separate subdirectory\n",
    "        self.val_log_dir = os.path.join(log_dir, 'validation')\n",
    "\n",
    "    def set_model(self, model):\n",
    "        # Setup writer for validation metrics\n",
    "        self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n",
    "        super(TrainValTensorBoard, self).set_model(model)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Pop the validation logs and handle them separately with\n",
    "        # `self.val_writer`. Also rename the keys so that they can\n",
    "        # be plotted on the same figure with the training metrics\n",
    "        logs = logs or {}\n",
    "        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if k.startswith('val_')}\n",
    "        for name, value in val_logs.items():\n",
    "            summary = tf.Summary()\n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = value.item()\n",
    "            summary_value.tag = name\n",
    "            self.val_writer.add_summary(summary, epoch)\n",
    "        self.val_writer.flush()\n",
    "\n",
    "        # Pass the remaining logs to `TensorBoard.on_epoch_end`\n",
    "        logs = {k: v for k, v in logs.items() if not k.startswith('val_')}\n",
    "        super(TrainValTensorBoard, self).on_epoch_end(epoch, logs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        super(TrainValTensorBoard, self).on_train_end(logs)\n",
    "        self.val_writer.close()\n",
    "\n",
    "#Instantiate it like so:\n",
    "tensorboard = TrainValTensorBoard(log_dir='/home/tuzunneslihan/MNIST/kerasfunc')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "history = model.fit(xTrain, yTrain, batch_size=32, nb_epoch=8,validation_data=(vTrain, vTest),shuffle=True, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0316 20:34:53.519562 140088041698688 training.py:686] The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33600 samples, validate on 28000 samples\n",
      "Epoch 1/8\n",
      "33600/33600 [==============================] - 13s 376us/sample - loss: 2.2968 - accuracy: 0.1265 - val_loss: 2.2822 - val_accuracy: 0.4253\n",
      "Epoch 2/8\n",
      "33600/33600 [==============================] - 9s 279us/sample - loss: 2.2883 - accuracy: 0.2034 - val_loss: 2.2760 - val_accuracy: 0.3932\n",
      "Epoch 3/8\n",
      "33600/33600 [==============================] - 9s 267us/sample - loss: 2.2764 - accuracy: 0.2753 - val_loss: 2.2644 - val_accuracy: 0.3746\n",
      "Epoch 4/8\n",
      "33600/33600 [==============================] - 9s 266us/sample - loss: 2.2586 - accuracy: 0.3258 - val_loss: 2.2453 - val_accuracy: 0.3602\n",
      "Epoch 5/8\n",
      "33600/33600 [==============================] - 10s 290us/sample - loss: 2.2310 - accuracy: 0.3624 - val_loss: 2.2182 - val_accuracy: 0.3339\n",
      "Epoch 6/8\n",
      "33600/33600 [==============================] - 9s 281us/sample - loss: 2.1880 - accuracy: 0.3999 - val_loss: 2.1799 - val_accuracy: 0.3082\n",
      "Epoch 7/8\n",
      "33600/33600 [==============================] - 9s 271us/sample - loss: 2.1232 - accuracy: 0.4333 - val_loss: 2.1296 - val_accuracy: 0.2847\n",
      "Epoch 8/8\n",
      "33600/33600 [==============================] - 9s 267us/sample - loss: 2.0320 - accuracy: 0.4643 - val_loss: 2.0826 - val_accuracy: 0.2486\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f681df21470>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRAINING\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "model.fit(xTrain, yTrain, batch_size=32, nb_epoch=8,validation_data=(vTrain, vTest),shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.zeros((num_testImages,2))\n",
    "for num in range(1,num_testImages + 1):\t\n",
    "    results[num - 1,0] = num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = model.predict(xTest) \n",
    "temp = y_prob.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in range(0,num_testImages):\t\n",
    "    results[num,1] = temp[num]\n",
    "# Results saved in this text file\n",
    "np.savetxt('submission2.csv', results, delimiter=',', fmt = '%i')  \n",
    "results = pd.np.array(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
